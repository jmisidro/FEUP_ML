{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.metrics import accuracy_score,precision_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>award_points</th>\n",
       "      <th>tmID</th>\n",
       "      <th>avg_height</th>\n",
       "      <th>avg_weight</th>\n",
       "      <th>avg_pl_efficiency</th>\n",
       "      <th>stint_coach</th>\n",
       "      <th>award_points_coach</th>\n",
       "      <th>mean_wins_coach</th>\n",
       "      <th>...</th>\n",
       "      <th>GP</th>\n",
       "      <th>homeW</th>\n",
       "      <th>homeL</th>\n",
       "      <th>awayW</th>\n",
       "      <th>awayL</th>\n",
       "      <th>confW</th>\n",
       "      <th>confL</th>\n",
       "      <th>min</th>\n",
       "      <th>post_W</th>\n",
       "      <th>post_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arcaija01w</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>25.1625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6475.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arcaija01w</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>162.500000</td>\n",
       "      <td>18.0250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arcaija01w</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>16.5075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6425.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arcaija01w</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>14.9040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6825.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arcaija01w</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  award_points tmID  avg_height  avg_weight  \\\n",
       "0  arcaija01w     1           0.0  HOU   72.333333  156.666667   \n",
       "1  arcaija01w     2           6.0  HOU   72.500000  162.500000   \n",
       "2  arcaija01w     3           0.0  HOU   73.500000  157.000000   \n",
       "3  arcaija01w     4           0.0  HOU   73.500000  157.000000   \n",
       "4  arcaija01w     5           0.0    0    0.000000    0.000000   \n",
       "\n",
       "   avg_pl_efficiency  stint_coach  award_points_coach  mean_wins_coach  ...  \\\n",
       "0            25.1625          0.0                 0.0         6.600000  ...   \n",
       "1            18.0250          0.0                 0.0         1.266667  ...   \n",
       "2            16.5075          0.0                 0.0         2.500000  ...   \n",
       "3            14.9040          0.0                 0.0         1.312500  ...   \n",
       "4             0.0000          0.0                 0.0         0.000000  ...   \n",
       "\n",
       "     GP  homeW  homeL awayW awayL confW  confL     min  post_W  post_L  \n",
       "0  32.0   14.0    2.0  13.0   3.0  17.0    4.0  6475.0     6.0     0.0  \n",
       "1  32.0   11.0    5.0   8.0   8.0  13.0    8.0  6450.0     0.0     2.0  \n",
       "2  32.0   14.0    2.0  10.0   6.0  16.0    5.0  6425.0     1.0     2.0  \n",
       "3  34.0   14.0    3.0   6.0  11.0  14.0   10.0  6825.0     1.0     2.0  \n",
       "4   NaN    NaN    NaN   NaN   NaN   NaN    NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/clean/merged.csv\")\n",
    "#df = pd.read_csv(\"../data/clean/teams.csv\") # TESTING\n",
    "\n",
    "# Convert \"playoff\" column to binary (Y: 1, N: 0)\n",
    "df[\"playoff\"] = df[\"playoff\"].map({\"Y\": 1, \"N\": 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>award_points</th>\n",
       "      <th>tmID</th>\n",
       "      <th>avg_height</th>\n",
       "      <th>avg_weight</th>\n",
       "      <th>avg_pl_efficiency</th>\n",
       "      <th>stint_coach</th>\n",
       "      <th>award_points_coach</th>\n",
       "      <th>mean_wins_coach</th>\n",
       "      <th>...</th>\n",
       "      <th>GP</th>\n",
       "      <th>homeW</th>\n",
       "      <th>homeL</th>\n",
       "      <th>awayW</th>\n",
       "      <th>awayL</th>\n",
       "      <th>confW</th>\n",
       "      <th>confL</th>\n",
       "      <th>min</th>\n",
       "      <th>post_W</th>\n",
       "      <th>post_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>25.1625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6475.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>162.500000</td>\n",
       "      <td>18.0250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>16.5075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6425.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>14.9040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6825.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>160.333333</td>\n",
       "      <td>15.1700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6875.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   playerID  year  award_points  tmID  avg_height  avg_weight  \\\n",
       "0         0     1           0.0     7   72.333333  156.666667   \n",
       "1         0     2           6.0     7   72.500000  162.500000   \n",
       "2         0     3           0.0     7   73.500000  157.000000   \n",
       "3         0     4           0.0     7   73.500000  157.000000   \n",
       "4         0     5           0.0     0    0.000000    0.000000   \n",
       "5         0     6           0.0     7   72.333333  160.333333   \n",
       "6         0     7           0.0     0    0.000000    0.000000   \n",
       "7         0     8           0.0     0    0.000000    0.000000   \n",
       "8         0     9           0.0     0    0.000000    0.000000   \n",
       "9         0    10           0.0     0    0.000000    0.000000   \n",
       "\n",
       "   avg_pl_efficiency  stint_coach  award_points_coach  mean_wins_coach  ...  \\\n",
       "0            25.1625          0.0                 0.0         6.600000  ...   \n",
       "1            18.0250          0.0                 0.0         1.266667  ...   \n",
       "2            16.5075          0.0                 0.0         2.500000  ...   \n",
       "3            14.9040          0.0                 0.0         1.312500  ...   \n",
       "4             0.0000          0.0                 0.0         0.000000  ...   \n",
       "5            15.1700          0.0                 0.0         1.166667  ...   \n",
       "6             0.0000          0.0                 0.0         0.000000  ...   \n",
       "7             0.0000          0.0                 0.0         0.000000  ...   \n",
       "8             0.0000          0.0                 0.0         0.000000  ...   \n",
       "9             0.0000          0.0                 0.0         0.000000  ...   \n",
       "\n",
       "     GP  homeW  homeL  awayW  awayL  confW  confL     min  post_W  post_L  \n",
       "0  32.0   14.0    2.0   13.0    3.0   17.0    4.0  6475.0     6.0     0.0  \n",
       "1  32.0   11.0    5.0    8.0    8.0   13.0    8.0  6450.0     0.0     2.0  \n",
       "2  32.0   14.0    2.0   10.0    6.0   16.0    5.0  6425.0     1.0     2.0  \n",
       "3  34.0   14.0    3.0    6.0   11.0   14.0   10.0  6825.0     1.0     2.0  \n",
       "4   NaN    NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN  \n",
       "5  34.0   11.0    6.0    8.0    9.0   11.0   11.0  6875.0     2.0     3.0  \n",
       "6   NaN    NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN  \n",
       "7   NaN    NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN  \n",
       "8   NaN    NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN  \n",
       "9   NaN    NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_df(df):\n",
    "    le = LabelEncoder()\n",
    "    tmID_mapping = {} \n",
    "    \n",
    "    for col, col_type in df.dtypes.items():\n",
    "        if col_type == 'object' or col_type == 'datetime64[ns]':\n",
    "            # store mapping if the column is 'tmID'\n",
    "            if col == 'tmID':\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "                tmID_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            else:\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "                \n",
    "    return df, tmID_mapping\n",
    "\n",
    "# Use the function to encode the dataframe and get the tmID mapping\n",
    "df, tmID_mapping = encode_df(df)\n",
    "\n",
    "\n",
    "df.head( 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_distribution_similarity(df, y_train, y_test):\n",
    "    # Step 1: Calculate class distribution in the original dataset\n",
    "    original_class_distribution = df['playoff'].value_counts(normalize=True)\n",
    "    print(\"Original Class Distribution:\")\n",
    "    print(original_class_distribution)\n",
    "\n",
    "    # Step 2: Calculate class distribution in the training and test sets\n",
    "    train_class_distribution = y_train.value_counts(normalize=True)\n",
    "    test_class_distribution = y_test.value_counts(normalize=True)\n",
    "\n",
    "    print(\"\\nTraining Set Class Distribution:\")\n",
    "    print(train_class_distribution)\n",
    "    print(\"\\nTest Set Class Distribution:\")\n",
    "    print(test_class_distribution)\n",
    "\n",
    "    # Step 3: Compare class distributions\n",
    "    class_distribution_similarity = (train_class_distribution - test_class_distribution).abs().sum()\n",
    "    print(\"\\nClass Distribution Similarity Score:\", class_distribution_similarity)\n",
    "\n",
    "    return class_distribution_similarity\n",
    "\n",
    "# Update the train_model function to include imputation\n",
    "def train_model(df, year):\n",
    "    # Remove rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    teams_df_train = df[df['year'] < year]\n",
    "    teams_df_test = df[df['year'] == year]\n",
    "\n",
    "    columns_to_keep = [\"playerID\", \"year\", \"award_points\", \"tmID\", \"avg_height\", \"avg_weight\", \"avg_pl_efficiency\", \"stint_coach\", \"award_points_coach\", \"mean_wins_coach\", \"confID\"]\n",
    "\n",
    "    X_train = teams_df_train[columns_to_keep]\n",
    "    y_train = teams_df_train[\"playoff\"]  # Target variable\n",
    "\n",
    "    X_test = teams_df_test[columns_to_keep]\n",
    "    y_test = teams_df_test[\"playoff\"]  # Target variable\n",
    "\n",
    "\n",
    "    print(f\"\\nTrain/Test size for year={year}:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# create a final dataset with the predictions: team, playoff, prediction (a probability between 0 and 1)\n",
    "def create_predictions(df, year, model, distribution_similarity=True):\n",
    "    # Create a copy of the original dataset\n",
    "    new_df = df.copy()\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_model(new_df, year)\n",
    "\n",
    "    # Check if the class distribution is similar between the training and test sets\n",
    "    if distribution_similarity:\n",
    "        calculate_class_distribution_similarity(new_df, y_train, y_test)    \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test) # Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test) # Prediction probabilities\n",
    "\n",
    "    # Only add predictions to the test set\n",
    "    new_df.loc[new_df['year'] == year, 'prediction'] = y_pred\n",
    "    new_df.loc[new_df['year'] == year, 'prediction_proba'] = y_pred_proba[:,1]\n",
    "\n",
    "    # Filter the dataset to only include the year we are interested in\n",
    "    new_df = new_df[new_df['year'] == year]\n",
    "\n",
    "    # Convert the predictions to integers\n",
    "    new_df['prediction'] = new_df['prediction'].astype(int)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: Logistic Regression\n",
      "\n",
      "Train/Test size for year=2: (24, 57) (31, 11) (24,) (31,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- GP\n- awayL\n- awayW\n- confL\n- confW\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m precision_scores[name] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_year, max_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution_similarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     auc_scores[name]\u001b[38;5;241m.\u001b[39mappend(roc_auc_score(new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayoff\u001b[39m\u001b[38;5;124m'\u001b[39m], new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_proba\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     30\u001b[0m     accuracy_scores[name]\u001b[38;5;241m.\u001b[39mappend(accuracy_score(new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayoff\u001b[39m\u001b[38;5;124m'\u001b[39m], new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[56], line 58\u001b[0m, in \u001b[0;36mcreate_predictions\u001b[0;34m(df, year, model, distribution_similarity)\u001b[0m\n\u001b[1;32m     55\u001b[0m     calculate_class_distribution_similarity(new_df, y_train, y_test)    \n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 58\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m     59\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test) \u001b[38;5;66;03m# Prediction probabilities\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Only add predictions to the test set\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:351\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 351\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    353\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:332\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    329\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    330\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 332\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- GP\n- awayL\n- awayW\n- confL\n- confW\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Test various models and see which one performs the best\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Ada Boost\": AdaBoostClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"Neural Network\": MLPClassifier()\n",
    "}\n",
    "\n",
    "min_year = 2\n",
    "max_year = 10\n",
    "\n",
    "# Create a dictionary to store the AUC, accuracy and precision scores for each model\n",
    "auc_scores = {}\n",
    "accuracy_scores = {}\n",
    "precision_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    auc_scores[name] = []\n",
    "    accuracy_scores[name] = []\n",
    "    precision_scores[name] = []\n",
    "\n",
    "    for i in range(min_year, max_year + 1):\n",
    "        new_df = create_predictions(df, i, model, distribution_similarity=False)\n",
    "        auc_scores[name].append(roc_auc_score(new_df['playoff'], new_df['prediction_proba']))\n",
    "        accuracy_scores[name].append(accuracy_score(new_df['playoff'], new_df['prediction']))\n",
    "        precision_scores[name].append(precision_score(new_df['playoff'], new_df['prediction']))\n",
    "\n",
    "# Plot the AUC scores for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, scores in auc_scores.items():\n",
    "    plt.plot(range(min_year, max_year + 1), scores, label=name)\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC Scores for Different Models\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy scores for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, scores in accuracy_scores.items():\n",
    "    plt.plot(range(min_year, max_year + 1), scores, label=name)\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Scores for Different Models\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the precision scores for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, scores in precision_scores.items():\n",
    "    plt.plot(range(min_year, max_year + 1), scores, label=name)\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision Scores for Different Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test size for year=2: (24, 57) (31, 57) (24,) (31,)\n",
      "\n",
      "Train/Test size for year=3: (55, 57) (41, 57) (55,) (41,)\n",
      "\n",
      "Train/Test size for year=4: (96, 57) (41, 57) (96,) (41,)\n",
      "\n",
      "Train/Test size for year=5: (137, 57) (43, 57) (137,) (43,)\n",
      "\n",
      "Train/Test size for year=6: (180, 57) (42, 57) (180,) (42,)\n",
      "\n",
      "Train/Test size for year=7: (222, 57) (35, 57) (222,) (35,)\n",
      "\n",
      "Train/Test size for year=8: (257, 57) (35, 57) (257,) (35,)\n",
      "\n",
      "Train/Test size for year=9: (292, 57) (37, 57) (292,) (37,)\n",
      "\n",
      "Train/Test size for year=10: (329, 57) (47, 57) (329,) (47,)\n"
     ]
    }
   ],
   "source": [
    "# Create predictions for the best model\n",
    "best_model = GradientBoostingClassifier() # An example of the best model\n",
    "\n",
    "def create_final_predictions(df, year):\n",
    "    # Select the best 4 teams for each conference (confID) ensuring unique teams\n",
    "    final_predictions = df[df['year'] == year].sort_values(by='prediction_proba', ascending=False).drop_duplicates(subset='tmID').groupby('confID').head(4)\n",
    "\n",
    "    # remove confID column\n",
    "    final_predictions.drop(['year', 'playoff', 'confID'], axis=1, inplace=True)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "# Add the tmID mapping to the final predictions so the final predictions have the team names\n",
    "def convert_tmID_to_team(df, tmID_mapping):\n",
    "    # Create a reverse mapping from the encoded numbers to the team names\n",
    "    reverse_mapping = {v: k for k, v in tmID_mapping.items()}\n",
    "    # Map the encoded tmID to the team names\n",
    "    df['tmID'] = df['tmID'].map(reverse_mapping)\n",
    "    return df\n",
    "\n",
    "# Usage of the function within the loop\n",
    "for i in range(min_year, max_year + 1):\n",
    "    new_df = create_predictions(df, i, best_model, distribution_similarity=False)\n",
    "    new_df = create_final_predictions(new_df, i)\n",
    "    new_df.drop(new_df.columns.difference(['tmID', 'confID', 'year', 'playoff', 'prediction', 'prediction_proba']), axis=1, inplace=True)\n",
    "    new_df = convert_tmID_to_team(new_df, tmID_mapping)\n",
    "    new_df.to_csv(f\"../data/predictions/predictions_{i}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
